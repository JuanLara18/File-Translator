"""
Translation Script

This script reads Excel (.xlsx) or Stata (.dta) files, translates specified columns from German
to English using the OpenAI API, and writes out a new file with all original data plus
additional columns containing English translations.

Usage:
    python enhanced_translation.py --input input_file.[xlsx|dta] --column German_Text [--output output_file.[xlsx|dta]] [--batch_size 10]

If the output file is not specified, it will be automatically generated by appending '_translated'
to the input file's name.
"""

import os
import sys
import argparse
import logging
import datetime
from typing import List, Dict, Any, Union, Tuple
from dotenv import load_dotenv
import pandas as pd
from tqdm import tqdm
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor
from functools import partial
import numpy as np
from threading import Lock
import os
import json


# -----------------------
# Configuration and Setup
# -----------------------

# Set up logging
#script_dir = os.path.dirname(os.path.abspath(__file__))
#logging.basicConfig(
#    filename=os.path.join(script_dir, 'translation.log'),
#    filemode='a',
#    level=logging.DEBUG,
#    format='%(asctime)s - %(levelname)s - %(message)s'
#)

# Load environment variables from .env file
load_dotenv("OPENAI_API_KEY.env")
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
if not OPENAI_API_KEY:
    logging.error("OpenAI API key not found in environment variables.")
    raise ValueError("OpenAI API key not found in environment variables. Please set it in your .env file.")

# Initialize OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY)

# Global constants
MODEL_NAME = 'gpt-3.5-turbo'

# -----------------------
# Functions
# -----------------------

def load_translation_cache(cache_file="translations_cache.json"):
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_translation_cache(cache, cache_file="translations_cache.json"):
    with open(cache_file, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def batch_translate_texts(texts: List[str], translation_cache: Dict[str, str] = None) -> Dict[str, str]:
    """
    Translates a batch of texts in parallel using ThreadPoolExecutor.
    
    Args:
        texts: List of unique texts to translate
        translation_cache: Optional cache dictionary for storing translations
    
    Returns:
        Dictionary with translations
    """
    if translation_cache is None:
        translation_cache = {}
    
    cache_lock = Lock()
    texts_to_translate = [text for text in texts if text not in translation_cache or not translation_cache[text]]
    
    if not texts_to_translate:
        return translation_cache
    
    def translate_single(text: str) -> tuple[str, str]:
        if not isinstance(text, str) or not text.strip():
            return text, text
        
        translated = translate_text(text)
        with cache_lock:
            translation_cache[text] = translated
        return text, translated

    with ThreadPoolExecutor(max_workers=10) as executor:
        list(executor.map(translate_single, texts_to_translate))
    
    return translation_cache

def extract_translation(response_text: str) -> tuple[str, bool]:
    """
    Extracts and validates translation from a response.
    Returns the translation and a boolean indicating if it's valid.
    
    :param response_text: The response text to validate
    :return: Tuple of (translation, is_valid_format)
    """
    try:
        # Si el texto está vacío o no es string, no es válido
        if not isinstance(response_text, str) or not response_text.strip():
            return response_text, False
            
        # Limpiar el texto de elementos comunes no deseados
        cleaned_text = (response_text.strip()
                       .replace('"', '')
                       .replace("'", "")
                       .replace(" translates to ", "")
                       .replace(" means ", "")
                       .replace(" in English", "")
                       .replace("Translation: ", ""))
        
        # Validaciones específicas para asegurar que es una traducción válida
        invalid_markers = [
            "means",
            "translates",
            "translation",
            "in English",
            "[original text]",
            "(German)",
            "German:",
            "English:",
            "=>",
            "->",
        ]
        
        # Si contiene marcadores de explicación, no es una traducción limpia
        if any(marker in cleaned_text.lower() for marker in invalid_markers):
            return response_text, False
            
        # Verificar que no sea igual al texto original si es una palabra común
        common_german_words = {"ja", "nein", "und", "oder", "der", "die", "das"}
        if cleaned_text.lower() in common_german_words:
            return response_text, False
            
        return cleaned_text, True
        
    except Exception as e:
        logging.error(f"Error extracting translation: {e}")
        return response_text, False

def create_translation_cache(df: pd.DataFrame, column: str) -> dict:
    """
    Creates a translation cache dictionary from unique values in the specified column.
    
    Args:
        df: Input DataFrame
        column: Column name containing texts to translate
    
    Returns:
        Dictionary with unique texts as keys and None as initial values
    """
    return {text: None for text in df[column].dropna().unique() if isinstance(text, str)}

def translate_text(text: str, translation_cache: dict = None) -> str:
    """
    Translates text from German to English, optimized for machine error descriptions.
    Uses caching to avoid redundant translations.
    
    :param text: A string in German to translate
    :param translation_cache: Cache of previous translations
    :return: The English translation as a string
    """
    if not isinstance(text, str) or not text.strip():
        return text

    system_message = """You are a technical translator specializing in machine error messages and technical descriptions. Follow these rules strictly:

1. Translate from German to English precisely and technically
2. Use consistent technical terminology
3. IMPORTANT: Only use the word "Error" if the German text contains "Fehler" or "Störung". Do not add the word "Error" if it's not in the original text
4. Keep all codes, numbers, and technical identifiers exactly as they appear (e.g., CC31 should stay CC31, not "Error CC31")
5. Maintain exact punctuation from the original text
6. Do not add any explanations or alternatives
7. Do not modify technical terms in brackets or parentheses
8. Preserve all variable names and placeholders exactly as they appear
9. NEVER add words that are not in the original text!

Example:
Input: "Fehler E123: Motorüberhitzung"
Output: Error E123: Motor overheating

Do not add any additional text, explanations, or the word 'Error' unless 'Fehler' or 'Störung' is present in the original text. Respond ONLY with the direct translation."""

    try:
        # Check cache first if provided
        if translation_cache is not None and text in translation_cache:
            return translation_cache[text]

        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": f"Translate: {text}"}
            ],
            temperature=0.1,
            max_tokens=1500  # Reduced as error messages are typically short
        )

        translated_text = response.choices[0].message.content.strip()
        
        # Clean the translation while preserving technical elements
        translated_text = (translated_text
                         .replace('"', '')
                         .replace("'", "")
                         .replace(" translates to ", "")
                         .replace(" means ", "")
                         .replace(" in English", "")
                         .replace("Translation: ", "")
                         .strip())

        # Update cache if provided
        if translation_cache is not None:
            translation_cache[text] = translated_text

        logging.info(f"Translated: {text} → {translated_text}")
        return translated_text

    except Exception as e:
        logging.error(f"Translation error for '{text}': {str(e)}")
        return text

def process_files(input_file: str, output_file: str, translate_col: str, 
                new_col_name: str = "Translation", batch_size: int = 10):
    """
    Memory-optimized version of process_files for handling large files efficiently.
    Only loads required columns and processes data in chunks.
    
    Args:
        input_file (str): Path to the input file (.xlsx, .xls, or .dta)
        output_file (str): Path where the translated file will be saved
        translate_col (str): Column name or letter to translate
        new_col_name (str): Name for the new translation column
        batch_size (int): Number of texts to process in each batch
    """

    try:
        CHUNK_SIZE = 100000  # Adjust based on available memory
        file_extension = os.path.splitext(input_file)[1].lower()

        # Initialize progress tracking
        with tqdm(total=100, desc="Overall Progress", position=0) as main_pbar:
            print("\nInitializing translation process...")
            persistent_cache = load_translation_cache()
            main_pbar.update(5)

            # First pass: Get column information and metadata
            print("\nAnalyzing file structure...")
            if file_extension == '.dta':
                # Read first chunk to get column information
                first_chunk = pd.read_stata(input_file, chunksize=1).__next__()
                column_names = first_chunk.columns.tolist()
                column_dtypes = first_chunk.dtypes.to_dict()
                
                # Get metadata
                stata_reader = pd.read_stata(input_file, iterator=True)
                stata_meta = {
                    'variable_labels': stata_reader.variable_labels(),
                    'value_labels': stata_reader.value_labels(),
                    'formats': column_dtypes  # Use the dtypes from the first chunk
                }
                stata_reader.close()
                
                # Count total rows without loading entire file
                print("Counting total rows...")
                total_rows = 0
                for chunk in pd.read_stata(input_file, chunksize=CHUNK_SIZE):
                    total_rows += len(chunk)
                    del chunk  # Free memory
                
            else:  # Excel files
                if file_extension in ['.xlsx', '.xls']:
                    # Read only the header to get column names
                    df_info = pd.read_excel(input_file, nrows=0)
                    column_names = df_info.columns.tolist()
                    # Get total rows without loading the file
                    total_rows = sum(1 for _ in pd.read_excel(input_file, usecols=[0]))
                stata_meta = None

            print(f"Found {total_rows:,} rows to process")

            # Validate column
            if translate_col not in column_names:
                try:
                    col_index = ord(translate_col.upper()) - ord('A')
                    if 0 <= col_index < len(column_names):
                        translate_col = column_names[col_index]
                    else:
                        raise ValueError(f"Column letter '{translate_col}' is out of range")
                except Exception as e:
                    raise ValueError(f"Invalid column identifier '{translate_col}'")
            main_pbar.update(10)

            # Second pass: Process only the required column in chunks
            print("\nCollecting unique texts for translation...")
            unique_texts = set()
            chunks_processed = 0
            
            # Function to read chunks efficiently based on file type
            def chunk_reader():
                if file_extension == '.dta':
                    return pd.read_stata(input_file, chunksize=CHUNK_SIZE, columns=[translate_col])
                else:  # Excel
                    return pd.read_excel(input_file, usecols=[translate_col], chunksize=CHUNK_SIZE)

            # Collect unique texts with progress tracking
            with tqdm(total=total_rows, desc="Reading texts", position=1) as read_pbar:
                for chunk in chunk_reader():
                    unique_texts.update(chunk[translate_col].dropna().unique())
                    read_pbar.update(len(chunk))
                    chunks_processed += 1

            main_pbar.update(15)

            # Create translation cache
            print("\nPreparing translation cache...")
            translation_cache = {text: persistent_cache.get(text, None) 
                              for text in unique_texts if isinstance(text, str)}
            
            cache_hits = sum(1 for v in translation_cache.values() if v is not None)
            print(f"Found {cache_hits} cached translations")
            main_pbar.update(10)

            # Process translations
            processed_translations = {}
            invalid_translations = []
            unique_texts_list = list(translation_cache.keys())
            
            total_batches = (len(unique_texts_list) + batch_size - 1) // batch_size
            print(f"\nProcessing {len(unique_texts_list)} unique texts in {total_batches} batches...")
            
            # Translation process (same as before)
            with tqdm(total=len(unique_texts_list), desc="Translating", position=1) as trans_pbar:
                for i in range(0, len(unique_texts_list), batch_size):
                    batch = unique_texts_list[i:i + batch_size]
                    batch_translations = batch_translate_texts(batch, processed_translations)
                    
                    for original, translated in batch_translations.items():
                        cleaned_translation, is_valid = extract_translation(translated)
                        if is_valid:
                            processed_translations[original] = cleaned_translation
                        else:
                            invalid_translations.append(original)
                            single_translation = translate_text(original)
                            retry_translation, retry_valid = extract_translation(single_translation)
                            if retry_valid:
                                processed_translations[original] = retry_translation
                            else:
                                processed_translations[original] = original
                        trans_pbar.update(1)
                    
                    main_pbar.update(30 / total_batches)

            # Save translation cache
            print("\nSaving translation cache...")
            save_translation_cache(translation_cache)
            main_pbar.update(5)

            # Third pass: Create output file with translations
            print(f"\nCreating translated file...")
            if file_extension == '.dta':
                # Initialize output Stata file
                first_chunk = True
                mode = 'w'
                
                with tqdm(total=total_rows, desc="Saving translations", position=1) as save_pbar:
                    for chunk in pd.read_stata(input_file, chunksize=CHUNK_SIZE, convert_categoricals=False):
                        # Create a copy of the chunk to avoid fragmentation
                        chunk_copy = chunk.copy()
                        
                        # Add translations to chunk and ensure proper string conversion
                        translations = chunk_copy[translate_col].map(processed_translations)
                        # Convert all None/NaN values to empty strings for Stata compatibility
                        translations = translations.fillna('').astype(str)
                        
                        # Ensure the translation column has a valid Stata variable name
                        valid_colname = new_col_name.replace(' ', '_').replace('-', '_')
                        chunk_copy[valid_colname] = translations
                        
                        # Convert any remaining problematic columns to string
                        for col in chunk_copy.select_dtypes(include=['object']).columns:
                            chunk_copy[col] = chunk_copy[col].fillna('').astype(str)
                        
                        # Convert datetime columns to string format
                        for col in chunk_copy.select_dtypes(include=['datetime64']).columns:
                            chunk_copy[col] = chunk_copy[col].dt.strftime('%Y-%m-%d')
                        
                        if first_chunk:
                            try:
                                # Write first chunk with metadata
                                if stata_meta:
                                    # Filter value_labels to only include existing columns
                                    filtered_value_labels = {
                                        key: value for key, value in stata_meta['value_labels'].items()
                                        if key in chunk_copy.columns
                                    }
                                    
                                    # Update variable labels with new column
                                    variable_labels = stata_meta['variable_labels'].copy()
                                    variable_labels[valid_colname] = f"English translation of: {variable_labels.get(translate_col, translate_col)}"
                                    
                                    # Write with filtered metadata
                                    chunk_copy.to_stata(
                                        output_file,
                                        write_index=False,
                                        variable_labels=variable_labels,
                                        value_labels=filtered_value_labels,
                                        version=118
                                    )
                                else:
                                    chunk_copy.to_stata(output_file, write_index=False, version=118)
                                mode = 'a'
                                first_chunk = False
                            except Exception as e:
                                print(f"\nError writing first chunk: {str(e)}")
                                print("\nAttempting to write without metadata...")
                                # Fallback: try writing without metadata
                                chunk_copy.to_stata(output_file, write_index=False, version=118)
                                mode = 'a'
                                first_chunk = False
                        else:
                            # Append subsequent chunks without metadata
                            chunk_copy.to_stata(output_file, mode=mode, write_index=False, version=118)
                        
                        save_pbar.update(len(chunk))
                        # Explicitly delete copies to free memory
                        del chunk_copy
                        del chunk

            else:  # Excel files
                writer = pd.ExcelWriter(output_file, engine='openpyxl')
                first_chunk = True
                
                with tqdm(total=total_rows, desc="Saving translations", position=1) as save_pbar:
                    for chunk in pd.read_excel(input_file, chunksize=CHUNK_SIZE):
                        # Add translations and ensure proper data types
                        translations = chunk[translate_col].map(processed_translations)
                        chunk[new_col_name] = translations.fillna('').astype(str)
                        
                        if first_chunk:
                            chunk.to_excel(writer, index=False, sheet_name='Sheet1')
                            first_chunk = False
                        else:
                            # Append mode for Excel
                            current_sheet = len(writer.sheets) + 1
                            chunk.to_excel(writer, sheet_name=f'Sheet{current_sheet}', index=False)
                        save_pbar.update(len(chunk))
                
                writer.close()

            main_pbar.update(20)

            # Log statistics
            print("\n" + "="*50)
            print("Translation Statistics:")
            print("="*50)
            print(f"Total rows processed:    {total_rows:,}")
            print(f"Unique texts translated: {len(unique_texts_list):,}")
            print(f"Cache hits:             {cache_hits:,}")
            print(f"Failed translations:     {len(invalid_translations):,}")
            success_rate = ((len(unique_texts_list) - len(invalid_translations)) / len(unique_texts_list) * 100) if unique_texts_list else 0
            print(f"Success rate:           {success_rate:.1f}%")
            print("="*50)

            return True, processed_translations

    except Exception as e:
        logging.error(f"Error processing file: {e}")
        print(f"\nError: {str(e)}")
        raise

def generate_output_filename(input_file: str) -> str:
    """
    Generates an output filename by appending '_translated' before the file extension.
    
    :param input_file: The input file name.
    :return: The generated output file name.
    """
    base, ext = os.path.splitext(input_file)
    return f"{base}_translated{ext}"

def parse_args():
    """
    Parses command-line arguments.
    """
    parser = argparse.ArgumentParser(
        description="Translate a specified column in the file from German to English."
    )
    parser.add_argument(
        "--input",
        required=True,
        help="Path to the input file."
    )
    parser.add_argument(
        "--output",
        help="Path to the output file. If not provided, the script will auto-generate one."
    )
    parser.add_argument(
        "--column",
        required=True,
        help="Column to translate (can be a column name or a letter, e.g., 'B')."
    )
    parser.add_argument(
        "--new_col",
        default="Translation",
        help="Name of the new column to hold the English translation. Default is 'Translation'."
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=10,
        help="Number of rows to process at a time. Default is 10."
    )
    return parser.parse_args()

# -----------------------
# Main Execution
# -----------------------

if __name__ == "__main__":
    args = parse_args()

    # Determine the output file name if not specified
    output_file = args.output if args.output else generate_output_filename(args.input)

    start_time = datetime.datetime.now()
    logging.info("Starting translation process.") 

    process_files(  # Changed from process_excel
        input_file=args.input,
        output_file=output_file,
        translate_col=args.column,
        new_col_name=args.new_col,
        batch_size=args.batch_size
    )

    end_time = datetime.datetime.now()
    elapsed_time = (end_time - start_time).total_seconds()
    print(f"Translation completed in {elapsed_time:.2f} seconds. Output saved to {output_file}.")
    logging.info(f"Translation process finished in {elapsed_time:.2f} seconds.")